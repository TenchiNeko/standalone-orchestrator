#!/usr/bin/env python3
"""
================================================================================
COLLABORATIVE SEQUENTIAL CODING AGENT v4.2.1 PROFESSIONAL EDITION
================================================================================

FIXED in v4.2.1:
- Bug fix: Manual :target paths now properly respected (no new project creation)
- Bug fix: Manager project_name no longer overrides manual targets
- Bug fix: Better code extraction for incomplete/truncated responses
- Bug fix: Path resolution handles both absolute and relative paths correctly

NEW in v4.2:
- Sequential multi-file generation with context tracking
- Context report system (CONTEXT_REPORT.md per project)
- File dependency tracking and integration awareness
- Progress visibility and resume capability
- Token management for large projects
- Final integration review with full context

FIXED in v4.1:
- Bug fix: Respects manual :target paths (doesn't create new projects)
- Bug fix: Better code block extraction (uses largest complete block)
- Bug fix: Handles both relative and absolute target paths

v4.0 Features:
- Project folder organization (auto-creates timestamped folders)
- Manual model selection (user chooses GLM or Qwen)
- Auto-testing with pytest
- Iterative refinement with manager feedback
- Template library for common tasks
- Comprehensive error handling & naming validation
"""

from __future__ import annotations
import os, re, sys, time, shutil, subprocess, logging, json, yaml
from dataclasses import dataclass
from typing import List, Optional, Dict, Tuple
from enum import Enum
from pathlib import Path
from datetime import datetime
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# Setup logging
LOG_DIR = Path.home() / ".collaborative_agent" / "logs"
LOG_DIR.mkdir(parents=True, exist_ok=True)
LOG_FILE = LOG_DIR / f"agent_pro_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def now_s(): return time.time()
def which(cmd): return shutil.which(cmd)
def die(msg, code=2): logger.error(msg); print(msg); raise SystemExit(code)

def normalize_host(url, default):
    url = (url or "").strip()
    if not url: url = default
    if url.endswith("/"): url = url[:-1]
    if not url.startswith("http"): die(f"Host must start with http:// : {url}")
    return url

def build_http():
    s = requests.Session()
    retry = Retry(total=4, backoff_factor=0.35, status_forcelist=(429,500,502,503,504))
    adapter = HTTPAdapter(max_retries=retry, pool_connections=10, pool_maxsize=10)
    s.mount("http://", adapter)
    s.mount("https://", adapter)
    return s

def sanitize_folder_name(name: str) -> str:
    """Sanitize project name for safe folder creation"""
    name = re.sub(r'[^\w\s-]', '', name)
    name = re.sub(r'\s+', '_', name)
    name = name.strip('_')
    name = name[:50]
    if not name:
        name = "project"
    return name.lower()

class ModelRole(Enum):
    MANAGER = "manager"
    CODER = "coder"

@dataclass
class TaskComplexity:
    """Analyze task complexity for informational purposes"""
    file_count: int
    estimated_lines: int
    has_external_deps: bool
    complexity_score: float
    
    @classmethod
    def analyze(cls, spec: str, tasks: str, targets: List[str]) -> 'TaskComplexity':
        file_count = len(targets)
        task_list = [t.strip() for t in tasks.split('\n') if t.strip() and t.strip()[0].isdigit()]
        estimated_lines = len(task_list) * 20
        spec_lower = spec.lower()
        has_external_deps = any(keyword in spec_lower for keyword in [
            'flask', 'fastapi', 'django', 'requests', 'pandas', 'numpy',
            'sqlalchemy', 'pytest', 'api', 'database', 'web'
        ])
        score = 0
        score += min(file_count * 15, 30)
        score += min(estimated_lines / 10, 40)
        score += 30 if has_external_deps else 0
        return cls(file_count, estimated_lines, has_external_deps, score)

class OllamaClient:
    def __init__(self, base_url, http, role):
        self.base_url = base_url
        self.http = http
        self.role = role
    
    def list_models(self):
        logger.info(f"Fetching models from {self.base_url}")
        r = self.http.get(f"{self.base_url}/api/tags", timeout=10)
        r.raise_for_status()
        models = [m.get("name") for m in r.json().get("models", []) if m.get("name")]
        logger.info(f"Found {len(models)} models on {self.role.value} host")
        return models
    
    def generate(self, model, prompt, timeout=240):
        token_limit = 6000 if self.role == ModelRole.MANAGER else 4000
        logger.debug(f"Generating with {model} (limit: {token_limit} tokens)")
        r = self.http.post(f"{self.base_url}/api/generate", json={
            "model": model, "prompt": prompt, "stream": False, "keep_alive": "10m",
            "options": {"temperature": 0.2, "top_p": 0.9, "num_predict": token_limit}
        }, timeout=timeout)
        if r.status_code != 200:
            logger.error(f"Ollama failed: {r.status_code}")
            raise RuntimeError(f"Ollama failed: {r.status_code}")
        return (r.json().get("response") or "").strip()
    
    def generate_stream(self, model, prompt, timeout=300):
        """Streaming generation with real-time output"""
        token_limit = 4000
        logger.debug(f"Streaming generation with {model} (limit: {token_limit} tokens)")
        
        r = self.http.post(f"{self.base_url}/api/generate", json={
            "model": model, "prompt": prompt, "stream": True, "keep_alive": "10m",
            "options": {"temperature": 0.2, "top_p": 0.9, "num_predict": token_limit}
        }, timeout=timeout, stream=True)
        
        if r.status_code != 200:
            logger.error(f"Ollama failed: {r.status_code}")
            raise RuntimeError(f"Ollama failed: {r.status_code}")
        
        full_response = []
        for line in r.iter_lines():
            if line:
                try:
                    chunk = json.loads(line)
                    if 'response' in chunk:
                        text = chunk['response']
                        full_response.append(text)
                        yield text
                    if chunk.get('done', False):
                        break
                except json.JSONDecodeError:
                    continue
        
        return ''.join(full_response)
    
    def warmup(self, model):
        try:
            logger.info(f"Warming up {self.role.value} model: {model}")
            print(f"  üî• Warming up {self.role.value} model: {model}")
            self.generate(model, "Hello", timeout=60)
            logger.info(f"{self.role.value} model ready")
            print(f"  ‚úì Model ready")
        except Exception as e:
            logger.warning(f"Warmup failed: {e}")

SPEC_RE = re.compile(r"BEGIN_SPEC\s*(.*?)\s*END_SPEC", re.S)
TASKS_RE = re.compile(r"BEGIN_CODER_TASKS\s*(.*?)\s*END_CODER_TASKS", re.S)
READY_RE = re.compile(r"PLAN_READY:\s*(YES|NO)\b", re.I)
TARGETS_RE = re.compile(r"TARGET_FILES:\s*(.*?)\s*(?:PLAN_READY:|$)", re.S)
PROJECT_NAME_RE = re.compile(r"PROJECT_NAME:\s*(.+?)(?:\n|$)", re.I)

@dataclass
class ParseResult:
    spec: str
    tasks: str
    plan_ready: bool
    targets: List[str]
    project_name: Optional[str]
    validation_errors: List[str]
    
    @property
    def is_valid(self): return len(self.validation_errors) == 0

def parse_manager_payload(text):
    spec = tasks = ""
    plan_ready = False
    targets = []
    project_name = None
    errors = []
    
    m = SPEC_RE.search(text or "")
    if m: spec = m.group(1).strip()
    if not spec: errors.append("Missing or empty SPEC")
    
    m = TASKS_RE.search(text or "")
    if m: tasks = m.group(1).strip()
    if not tasks: errors.append("Missing or empty TASKS")
    
    m = READY_RE.search(text or "")
    if m: plan_ready = m.group(1).upper() == "YES"
    
    m = TARGETS_RE.search(text or "")
    if m:
        for line in m.group(1).splitlines():
            line = line.strip().lstrip("-").strip()
            if line and not line.lower().startswith("none"):
                targets.append(line)
    
    m = PROJECT_NAME_RE.search(text or "")
    if m: project_name = m.group(1).strip()
    
    if plan_ready and (not spec or not tasks or not targets):
        errors.append("PLAN_READY:YES but missing content")
    
    logger.debug(f"Parsed manager output: ready={plan_ready}, targets={targets}, project={project_name}, errors={errors}")
    return ParseResult(spec, tasks, plan_ready, targets, project_name, errors)

@dataclass
class ConversationTurn:
    role: str
    content: str
    timestamp: float

class ConversationHistory:
    def __init__(self, max_turns=20):
        self.turns = []
        self.max_turns = max_turns
    
    def add_user(self, content):
        self.turns.append(ConversationTurn("user", content, now_s()))
        if len(self.turns) > self.max_turns: self.turns = self.turns[-self.max_turns:]
        logger.debug(f"Added user turn: {content[:50]}...")
    
    def add_assistant(self, content):
        self.turns.append(ConversationTurn("assistant", content, now_s()))
        if len(self.turns) > self.max_turns: self.turns = self.turns[-self.max_turns:]
        logger.debug(f"Added assistant turn: {content[:50]}...")
    
    def get_context(self, max_chars=8000):
        if not self.turns: return ""
        lines = ["Previous conversation:"]
        total = 0
        for turn in reversed(self.turns):
            txt = f"\n{turn.role.upper()}: {turn.content}"
            if total + len(txt) > max_chars: break
            lines.insert(1, txt)
            total += len(txt)
        return "\n".join(lines) if len(lines) > 1 else ""
    
    def clear(self): 
        logger.info("Clearing conversation history")
        self.turns.clear()

@dataclass
class Config:
    manager_host: str
    coder_host: str
    manager_model: str
    default_coder_model: str
    glm_model: str
    qwen_model: str
    projects_dir: Path
    templates_dir: Path

class ContextReportManager:
    """Manages project context across multi-file generation"""
    
    def __init__(self, project_path: Path):
        self.project_path = project_path
        self.report_path = project_path / "CONTEXT_REPORT.md"
        self.completed_files = []
        self.pending_files = []
        self.current_file = None
        self.dependencies = {}
        self.integration_notes = []
        self.project_name = project_path.name
        self.start_time = datetime.now()
    
    def initialize(self, project_name: str, spec: str, targets: List[str]):
        """Create initial context report from manager's plan"""
        
        self.project_name = project_name
        self.pending_files = targets.copy()
        
        report = f"""# Project Context Report: {project_name}

**Generated:** {self.start_time.strftime("%Y-%m-%d %H:%M:%S")}
**Status:** INITIALIZED (0/{len(targets)} files complete)

## Project Overview
{spec}

## Architecture
{self._format_file_list(targets)}

## Completed Files ‚úÖ
(None yet)

## In Progress üîÑ
### {targets[0]}
- Next to generate
- Status: PENDING

## Pending ‚è≥
{self._format_pending(targets[1:])}

## Integration Notes
(Will be updated as files are generated)

## Known Issues
(None yet)

## Next Steps
1. Generate {targets[0]}
2. Update context report
3. Continue to next file
"""
        self.report_path.write_text(report)
        logger.info(f"Initialized context report: {len(targets)} files")
    
    def _format_file_list(self, targets: List[str]) -> str:
        """Format file list with basic structure"""
        lines = []
        for target in targets:
            lines.append(f"- {target}")
        return "\n".join(lines)
    
    def _format_pending(self, targets: List[str]) -> str:
        """Format pending files list"""
        if not targets:
            return "(All files generated)"
        return "\n".join(f"- {t}" for t in targets)
    
    def mark_file_complete(self, filename: str, file_info: Dict):
        """Mark a file as complete and update context"""
        
        self.completed_files.append({
            'filename': filename,
            'size': file_info.get('size', 0),
            'functions': file_info.get('functions', []),
            'classes': file_info.get('classes', []),
            'dependencies': file_info.get('dependencies', []),
            'exports': file_info.get('exports', []),
            'timestamp': datetime.now()
        })
        
        if filename in self.pending_files:
            self.pending_files.remove(filename)
        
        self._update_report()
        logger.info(f"Marked {filename} complete in context report")
    
    def add_integration_note(self, note: str):
        """Add a note about file integration"""
        self.integration_notes.append({
            'note': note,
            'timestamp': datetime.now()
        })
        self._update_report()
    
    def get_context_for_file(self, filename: str) -> str:
        """Get relevant context for generating a specific file"""
        
        context_lines = [
            f"## Context for {filename}",
            "",
            "### Already Implemented:",
        ]
        
        if not self.completed_files:
            context_lines.append("(No files completed yet - this is the first file)")
        else:
            for file_info in self.completed_files:
                funcs = file_info['functions'][:5]
                classes = file_info['classes'][:3]
                context_lines.append(f"- **{file_info['filename']}** ({file_info['size']} bytes)")
                if funcs:
                    context_lines.append(f"  - Functions: {', '.join(funcs)}")
                if classes:
                    context_lines.append(f"  - Classes: {', '.join(classes)}")
        
        context_lines.extend([
            "",
            "### Available for Import:",
        ])
        
        if self.completed_files:
            for file_info in self.completed_files:
                module_name = file_info['filename'].replace('.py', '')
                exports = file_info['exports'][:5]
                if exports:
                    context_lines.append(f"- `from {module_name} import {', '.join(exports)}`")
        else:
            context_lines.append("(No modules available yet)")
        
        context_lines.extend([
            "",
            "### Integration Requirements:",
            "- Maintain consistent error handling (try/except with logging)",
            "- Include comprehensive docstrings for all public functions",
            "- Follow established patterns from completed files",
            "- Add type hints where appropriate",
        ])
        
        return "\n".join(context_lines)
    
    def _update_report(self):
        """Regenerate the full context report"""
        
        total = len(self.completed_files) + len(self.pending_files)
        completed_count = len(self.completed_files)
        
        if completed_count == total:
            status = f"COMPLETE ({completed_count}/{total} files)"
        else:
            status = f"IN_PROGRESS ({completed_count}/{total} files complete)"
        
        report_lines = [
            f"# Project Context Report: {self.project_name}",
            f"",
            f"**Started:** {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}",
            f"**Last Updated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"**Status:** {status}",
            f"",
        ]
        
        # Progress bar
        progress_pct = (completed_count / total * 100) if total > 0 else 0
        progress_bar = "‚ñà" * int(progress_pct / 5) + "‚ñë" * (20 - int(progress_pct / 5))
        report_lines.extend([
            f"## Progress",
            f"```",
            f"[{progress_bar}] {progress_pct:.0f}%",
            f"```",
            f""
        ])
        
        # Completed files
        report_lines.append(f"## Completed Files ‚úÖ")
        
        if not self.completed_files:
            report_lines.append("(None yet)")
        else:
            for file_info in self.completed_files:
                elapsed = (file_info['timestamp'] - self.start_time).total_seconds()
                report_lines.extend([
                    f"",
                    f"### {file_info['filename']} ({file_info['size']} bytes)",
                    f"- **Completed:** {file_info['timestamp'].strftime('%H:%M:%S')} (+{elapsed:.0f}s)",
                ])
                
                if file_info['functions']:
                    report_lines.append(f"- **Functions:** {', '.join(file_info['functions'])}")
                if file_info['classes']:
                    report_lines.append(f"- **Classes:** {', '.join(file_info['classes'])}")
                if file_info['dependencies']:
                    report_lines.append(f"- **Dependencies:** {', '.join(file_info['dependencies'])}")
        
        report_lines.append("")
        
        # Pending files
        if self.pending_files:
            report_lines.extend([
                f"## Pending ‚è≥",
                f"",
            ])
            for pf in self.pending_files:
                report_lines.append(f"- {pf}")
            report_lines.append("")
        
        # Integration notes
        if self.integration_notes:
            report_lines.extend([
                f"## Integration Notes",
                f"",
            ])
            for note_info in self.integration_notes:
                time_str = note_info['timestamp'].strftime('%H:%M:%S')
                report_lines.append(f"- [{time_str}] {note_info['note']}")
            report_lines.append("")
        
        # Next steps
        report_lines.extend([
            f"## Next Steps",
        ])
        
        if not self.pending_files:
            report_lines.extend([
                f"1. ‚úÖ All files generated",
                f"2. Run integration tests",
                f"3. Final review and deployment prep",
            ])
        else:
            next_file = self.pending_files[0]
            report_lines.extend([
                f"1. Generate `{next_file}`",
                f"2. Update context report",
                f"3. Continue to next file ({len(self.pending_files)-1} remaining)",
            ])
        
        self.report_path.write_text("\n".join(report_lines))

class TemplateLibrary:
    """Pre-built templates for common tasks"""
    
    def __init__(self, templates_dir: Path):
        self.templates_dir = templates_dir
        self.templates_dir.mkdir(parents=True, exist_ok=True)
        self._initialize_default_templates()
    
    def _initialize_default_templates(self):
        """Create default templates if they don't exist"""
        
        web_api_template = {
            "name": "web_api",
            "description": "RESTful API with Flask",
            "prompt_template": "Create a Flask REST API for {resource} with CRUD operations, SQLAlchemy ORM, error handling, and API documentation.",
            "files": ["app.py", "models.py", "routes.py", "config.py", "requirements.txt"],
            "complexity": "medium"
        }
        
        cli_tool_template = {
            "name": "cli_tool",
            "description": "Command-line tool with argparse",
            "prompt_template": "Create a CLI tool for {purpose} using argparse with subcommands, help text, and proper error handling.",
            "files": ["cli.py", "utils.py", "config.py"],
            "complexity": "simple"
        }
        
        data_pipeline_template = {
            "name": "data_pipeline",
            "description": "ETL data pipeline",
            "prompt_template": "Create a data pipeline for {source} to {destination} with pandas, data validation, error logging, and scheduling.",
            "files": ["pipeline.py", "extractors.py", "transformers.py", "loaders.py", "config.py"],
            "complexity": "complex"
        }
        
        scraper_template = {
            "name": "scraper",
            "description": "Web scraper with Beautiful Soup",
            "prompt_template": "Create a web scraper for {target_site} using requests and BeautifulSoup with rate limiting, error handling, and data export.",
            "files": ["scraper.py", "parser.py", "storage.py"],
            "complexity": "medium"
        }
        
        templates = [web_api_template, cli_tool_template, data_pipeline_template, scraper_template]
        
        for template in templates:
            template_file = self.templates_dir / f"{template['name']}.yaml"
            if not template_file.exists():
                with open(template_file, 'w') as f:
                    yaml.dump(template, f)
                logger.info(f"Created template: {template['name']}")
    
    def list_templates(self) -> List[Dict]:
        """List all available templates"""
        templates = []
        for template_file in self.templates_dir.glob("*.yaml"):
            try:
                with open(template_file) as f:
                    templates.append(yaml.safe_load(f))
            except Exception as e:
                logger.warning(f"Failed to load template {template_file}: {e}")
        return templates
    
    def get_template(self, name: str) -> Optional[Dict]:
        """Get a specific template by name"""
        template_file = self.templates_dir / f"{name}.yaml"
        if template_file.exists():
            with open(template_file) as f:
                return yaml.safe_load(f)
        return None
    
    def apply_template(self, name: str, **kwargs) -> str:
        """Apply a template with given parameters"""
        template = self.get_template(name)
        if not template:
            return None
        
        try:
            prompt = template['prompt_template'].format(**kwargs)
            return prompt
        except KeyError as e:
            logger.error(f"Missing template parameter: {e}")
            return None

class ProjectManager:
    """Manages project folder structure and organization"""
    
    def __init__(self, projects_dir: Path):
        self.projects_dir = projects_dir
        self.projects_dir.mkdir(parents=True, exist_ok=True)
        self.current_project = None
    
    def create_project(self, name: str, user_request: str) -> Path:
        """Create a new project folder with timestamp"""
        clean_name = sanitize_folder_name(name) if name else "unnamed_project"
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        folder_name = f"{clean_name}_{timestamp}"
        project_path = self.projects_dir / folder_name
        
        counter = 1
        while project_path.exists():
            folder_name = f"{clean_name}_{timestamp}_{counter}"
            project_path = self.projects_dir / folder_name
            counter += 1
        
        project_path.mkdir(parents=True, exist_ok=True)
        logger.info(f"Created project folder: {project_path}")
        
        readme_content = f"""# {clean_name.replace('_', ' ').title()}

**Created:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

## User Request
{user_request}

## Files
(Files will be listed here after generation)

## Generated by
Collaborative Coding Agent v4.2.1
"""
        (project_path / "README.md").write_text(readme_content)
        
        gitignore_content = """__pycache__/
*.py[cod]
*$py.class
*.so
.Python
venv/
ENV/
.env
.vscode/
.idea/
*.log
CONTEXT_REPORT.md
"""
        (project_path / ".gitignore").write_text(gitignore_content)
        
        self.current_project = project_path
        return project_path
    
    def get_project_path(self, *parts) -> Path:
        """Get path within current project"""
        if not self.current_project:
            raise RuntimeError("No active project")
        return self.current_project / Path(*parts)
    
    def update_readme(self, files: List[str], test_results: Optional[str] = None):
        """Update README with generated files and test results"""
        if not self.current_project:
            return
        
        readme_path = self.current_project / "README.md"
        current_content = readme_path.read_text()
        
        files_section = "\n## Files\n" + "\n".join(f"- {f}" for f in files)
        
        if test_results:
            test_section = f"\n## Test Results\n```\n{test_results}\n```"
        else:
            test_section = ""
        
        if "(Files will be listed here" in current_content:
            current_content = current_content.replace(
                "## Files\n(Files will be listed here after generation)",
                files_section
            )
        else:
            current_content += files_section
        
        current_content += test_section
        readme_path.write_text(current_content)

class RalphWiggumLoop:
    """
    Autonomous 'Ralph Wiggum' loop for persistent task completion.

    The Ralph Wiggum pattern shifts from chatbot (one-shot) to agent (iterate until done).
    Components:
    - Progress Tracker: Reads progress.txt and prd.json at each iteration
    - Completion Promise: Exits only when <promise>COMPLETE</promise> is output
    - Safety Valve: Max iterations to prevent runaway execution
    """

    PROMISE_COMPLETE = "<promise>COMPLETE</promise>"
    PROGRESS_FILE = "progress.txt"
    PRD_FILE = "prd.json"

    def __init__(self, project_path: Path, max_iterations: int = 20):
        self.project_path = project_path
        self.max_iterations = max_iterations
        self.current_iteration = 0
        self.progress_path = project_path / self.PROGRESS_FILE
        self.prd_path = project_path / self.PRD_FILE
        logger.info(f"RalphWiggumLoop initialized: max_iterations={max_iterations}")

    def initialize_progress(self, task_description: str, tasks: List[Dict] = None):
        """Initialize progress tracking files for a new Ralph loop"""

        # Initialize progress.txt
        progress_content = f"""# Ralph Wiggum Loop Progress
# Task: {task_description}
# Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Current Status
- Iteration: 0
- Phase: INITIALIZED
- Last Action: None
- Blockers: None

## History
[{datetime.now().strftime('%H:%M:%S')}] Loop initialized
"""
        self.progress_path.write_text(progress_content)
        logger.info(f"Initialized {self.PROGRESS_FILE}")

        # Initialize prd.json if tasks provided
        if tasks:
            prd_content = {
                "task": task_description,
                "created": datetime.now().isoformat(),
                "tasks": tasks,
                "completion_percentage": 0
            }
        else:
            prd_content = {
                "task": task_description,
                "created": datetime.now().isoformat(),
                "tasks": [
                    {"id": 1, "description": "Write failing tests", "completed": False},
                    {"id": 2, "description": "Implement code to pass tests", "completed": False},
                    {"id": 3, "description": "Verify all tests pass", "completed": False},
                    {"id": 4, "description": "Final review and cleanup", "completed": False}
                ],
                "completion_percentage": 0
            }

        with open(self.prd_path, 'w') as f:
            json.dump(prd_content, f, indent=2)
        logger.info(f"Initialized {self.PRD_FILE}")

        return progress_content, prd_content

    def read_progress(self) -> Tuple[str, Dict]:
        """Read current progress state from tracking files"""

        progress_text = ""
        prd_data = {}

        if self.progress_path.exists():
            progress_text = self.progress_path.read_text()
        else:
            logger.warning(f"{self.PROGRESS_FILE} not found")

        if self.prd_path.exists():
            try:
                with open(self.prd_path) as f:
                    prd_data = json.load(f)
            except json.JSONDecodeError as e:
                logger.error(f"Failed to parse {self.PRD_FILE}: {e}")
        else:
            logger.warning(f"{self.PRD_FILE} not found")

        return progress_text, prd_data

    def update_progress(self, status: str, action: str, blockers: str = "None", phase: str = "WORKING"):
        """Update progress.txt with current state"""

        progress_text, _ = self.read_progress()

        # Update the status section
        lines = progress_text.split('\n')
        new_lines = []
        in_status = False

        for line in lines:
            if line.startswith('## Current Status'):
                in_status = True
                new_lines.append(line)
                new_lines.append(f"- Iteration: {self.current_iteration}")
                new_lines.append(f"- Phase: {phase}")
                new_lines.append(f"- Last Action: {action}")
                new_lines.append(f"- Blockers: {blockers}")
                continue

            if in_status and line.startswith('- '):
                continue

            if in_status and (line.startswith('## ') or line == ''):
                in_status = False

            if not in_status:
                new_lines.append(line)

        # Add history entry
        history_entry = f"[{datetime.now().strftime('%H:%M:%S')}] Iter {self.current_iteration}: {action}"

        for i, line in enumerate(new_lines):
            if line.startswith('## History'):
                new_lines.insert(i + 1, history_entry)
                break

        self.progress_path.write_text('\n'.join(new_lines))
        logger.debug(f"Updated progress: {action}")

    def update_prd_task(self, task_id: int, completed: bool):
        """Mark a task as completed in prd.json"""

        _, prd_data = self.read_progress()

        if not prd_data or 'tasks' not in prd_data:
            logger.warning("No PRD data to update")
            return

        for task in prd_data['tasks']:
            if task.get('id') == task_id:
                task['completed'] = completed
                break

        # Update completion percentage
        total = len(prd_data['tasks'])
        completed_count = sum(1 for t in prd_data['tasks'] if t.get('completed'))
        prd_data['completion_percentage'] = int((completed_count / total) * 100) if total > 0 else 0

        with open(self.prd_path, 'w') as f:
            json.dump(prd_data, f, indent=2)

        logger.info(f"PRD updated: task {task_id} = {completed}, {prd_data['completion_percentage']}% complete")

    def is_complete(self, prd_data: Dict) -> bool:
        """Check if all tasks in PRD are complete"""

        if not prd_data or 'tasks' not in prd_data:
            return False

        return all(task.get('completed', False) for task in prd_data['tasks'])

    def check_safety_valve(self) -> bool:
        """Check if we've exceeded max iterations"""

        if self.current_iteration >= self.max_iterations:
            logger.warning(f"Safety valve triggered: {self.current_iteration} >= {self.max_iterations}")
            return True
        return False

    def get_loop_prompt(self, task: str, context: str = "") -> str:
        """Generate the Ralph Wiggum loop prompt for the AI"""

        progress_text, prd_data = self.read_progress()

        return f"""You are operating in an autonomous 'Ralph Wiggum' loop. Your goal is not to finish in one turn, but to iterate until the task is verified complete.

CURRENT STATE (Iteration {self.current_iteration}/{self.max_iterations}):
--- progress.txt ---
{progress_text}
--- end progress.txt ---

--- prd.json ---
{json.dumps(prd_data, indent=2)}
--- end prd.json ---

TASK: {task}

{context}

INSTRUCTIONS:
1. Track Progress: You have read the current state above. Analyze what has been done and what remains.
2. Execute TDD: Write a failing test first, then implement the code, then run the test.
3. Persistence: If a test fails or a command errors, do NOT ask for help. Analyze the error, fix the code, and try again.
4. Update State: Before ending your response, specify updates for progress.txt with your current status.
5. Exit Condition: Only output the exact string {self.PROMISE_COMPLETE} when ALL tests pass AND the task list in prd.json is 100% checked off.

RESPONSE FORMAT:
1. Analysis of current state
2. Action to take this iteration
3. Code/implementation
4. Test results
5. Progress update (what to write to progress.txt)
6. If ALL tasks complete and verified: {self.PROMISE_COMPLETE}

Begin your iteration now:"""


class TestRunner:
    """Automatically run pytest on generated code"""

    def __init__(self, project_path: Path):
        self.project_path = project_path
    
    def has_tests(self) -> bool:
        """Check if project has test files"""
        test_files = list(self.project_path.glob("test_*.py"))
        test_files += list(self.project_path.glob("*_test.py"))
        test_files += list((self.project_path / "tests").glob("*.py")) if (self.project_path / "tests").exists() else []
        return len(test_files) > 0
    
    def run_tests(self) -> Tuple[bool, str]:
        """Run pytest and return (success, output)"""
        if not which("pytest"):
            return False, "pytest not installed. Run: pip install pytest"
        
        if not self.has_tests():
            return True, "No tests found (skipped)"
        
        try:
            result = subprocess.run(
                ["pytest", "-v", "--tb=short"],
                cwd=self.project_path,
                capture_output=True,
                text=True,
                timeout=60
            )
            
            output = result.stdout + result.stderr
            success = result.returncode == 0
            
            return success, output
        except subprocess.TimeoutExpired:
            return False, "Tests timed out after 60 seconds"
        except Exception as e:
            return False, f"Test execution failed: {e}"

class Prerequisites:
    """Validates prerequisites before execution"""
    
    @staticmethod
    def check_git_repo(path: Path) -> tuple[bool, str]:
        """Check if directory is a git repo"""
        try:
            result = subprocess.run(
                ['git', 'rev-parse', '--git-dir'],
                cwd=path,
                capture_output=True,
                text=True,
                timeout=5
            )
            if result.returncode == 0:
                return True, "Git repository found"
            else:
                return False, "Not a git repository"
        except Exception as e:
            return False, f"Git not available: {e}"
    
    @staticmethod
    def check_target_files(project_path: Path, targets: List[str]) -> tuple[bool, str]:
        """Check if target files can be created"""
        for target in targets:
            target_path = project_path / target
            parent = target_path.parent
            if not parent.exists():
                parent.mkdir(parents=True, exist_ok=True)
            if not os.access(parent, os.W_OK):
                return False, f"No write permission: {parent}"
        return True, "All target files accessible"
    
    @staticmethod
    def validate_all(project_path: Path, targets: List[str]) -> tuple[bool, List[str]]:
        """Run all prerequisite checks"""
        issues = []
        
        git_ok, git_msg = Prerequisites.check_git_repo(project_path)
        if not git_ok:
            issues.append(f"‚ö†Ô∏è  Git: {git_msg}")
        else:
            issues.append(f"‚úì Git: {git_msg}")
        
        files_ok, files_msg = Prerequisites.check_target_files(project_path, targets)
        if not files_ok:
            issues.append(f"‚ùå Files: {files_msg}")
        else:
            issues.append(f"‚úì Files: {files_msg}")
        
        all_ok = files_ok
        return all_ok, issues

class CollaborativeAgentPro:
    def __init__(self):
        logger.info("Initializing Collaborative Agent v4.2.1 (Professional Edition)")
        http = build_http()
        self.http = http
        self.cfg = self._load_config()
        self.manager = OllamaClient(self.cfg.manager_host, http, ModelRole.MANAGER)
        self.coder = OllamaClient(self.cfg.coder_host, http, ModelRole.CODER)
        self._verify()
        
        self.project_manager = ProjectManager(self.cfg.projects_dir)
        self.template_library = TemplateLibrary(self.cfg.templates_dir)
        self.history = ConversationHistory()
        
        self.last_manager_text = None
        self.last_parsed = None
        self.targets = []
        self.current_model = None
        self.refinement_round = 0
        self.manual_target_set = False
        
        print(f"\nüìã Logs: {LOG_FILE}")
        print(f"üìÅ Projects: {self.cfg.projects_dir}")
        print(f"üìÑ Templates: {len(self.template_library.list_templates())} available")
    
    def _load_config(self):
        base_dir = Path.home() / ".collaborative_agent"
        base_dir.mkdir(parents=True, exist_ok=True)
        
        glm = os.getenv("GLM_MODEL", "glm-4.7-flash:latest").strip()
        qwen = os.getenv("QWEN_MODEL", "qwen2.5-coder:32b-instruct-q8_0").strip()
        default = os.getenv("DEFAULT_CODER_MODEL", glm).strip()
        
        cfg = Config(
            manager_host=normalize_host(os.getenv("MANAGER_OLLAMA_HOST", ""), "http://localhost:11434"),
            coder_host=normalize_host(os.getenv("CODER_OLLAMA_HOST", ""), "http://localhost:11434"),
            manager_model=os.getenv("MANAGER_MODEL", "").strip(),
            default_coder_model=default,
            glm_model=glm,
            qwen_model=qwen,
            projects_dir=Path(os.getenv("PROJECTS_DIR", "./projects")).resolve(),
            templates_dir=base_dir / "templates"
        )
        
        cfg.projects_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"Config loaded: default_coder={default}")
        return cfg
    
    def _verify(self):
        print("\nüîç Verifying setup...")
        logger.info("Starting setup verification")
        
        try:
            mgr_models = self.manager.list_models()
        except Exception as e:
            die(f"‚ùå Manager host unreachable: {self.cfg.manager_host}\n{e}")
        
        try:
            cod_models = self.coder.list_models()
        except Exception as e:
            die(f"‚ùå Coder host unreachable: {self.cfg.coder_host}\n{e}")
        
        if not self.cfg.manager_model:
            for pref in ["llama3.2:latest", "llama3:8b", "llama3:latest"]:
                if pref in mgr_models:
                    self.cfg.manager_model = pref
                    break
        
        if self.cfg.manager_model not in mgr_models:
            die(f"‚ùå Manager model not found: {self.cfg.manager_model}\nAvailable: {mgr_models[:5]}")
        
        has_glm = any(self.cfg.glm_model in m for m in cod_models)
        has_qwen = any(self.cfg.qwen_model in m for m in cod_models)
        
        print(f"‚úì Manager: {self.cfg.manager_host} ({self.cfg.manager_model})")
        print(f"‚úì Default Coder: {self.cfg.default_coder_model}")
        print(f"  ‚Ä¢ GLM: {self.cfg.glm_model} {'‚úì' if has_glm else '‚úó (not installed)'}")
        print(f"  ‚Ä¢ Qwen: {self.cfg.qwen_model} {'‚úì' if has_qwen else '‚úó (not installed)'}")
        
        if not has_glm and not has_qwen:
            die("‚ùå No coder models available. Install GLM or Qwen.")
        
        if self.cfg.default_coder_model not in cod_models:
            print(f"‚ö†Ô∏è  Default model not found: {self.cfg.default_coder_model}")
            if has_glm:
                self.cfg.default_coder_model = self.cfg.glm_model
            elif has_qwen:
                self.cfg.default_coder_model = self.cfg.qwen_model
            print(f"   Using fallback: {self.cfg.default_coder_model}")
        
        print("\nüî• Warming up manager model...")
        self.manager.warmup(self.cfg.manager_model)
        logger.info("Setup verification complete")
    
    def manager_prompt(self, user_text: str) -> str:
        ctx = self.history.get_context()
        
        base_prompt = f"""You are the MANAGER. Create executable plans.

OUTPUT FORMAT:
PROJECT_NAME: short_descriptive_name

BEGIN_SPEC
## Requirements
- [specific requirements]
## Implementation
- [approach, files]
## Acceptance
- [how to verify]
END_SPEC

BEGIN_CODER_TASKS
1. [specific task with filenames]
2. [continue...]
END_CODER_TASKS

TARGET_FILES:
- file.py

PLAN_READY: YES/NO

RULES:
- PROJECT_NAME should be short, descriptive, snake_case
- Only what user requested, no extras
- Specific filenames and functions
- PLAN_READY:YES only when complete

{ctx}

USER: {user_text}"""
        
        return base_prompt
    
    def ask_manager(self, user_text):
        print("\nüß† Manager thinking...")
        logger.info("Asking manager")
        t0 = now_s()
        out = self.manager.generate(self.cfg.manager_model, self.manager_prompt(user_text), 300)
        elapsed = now_s() - t0
        print(f"‚úì Responded in {elapsed:.1f}s")
        logger.info(f"Manager responded in {elapsed:.1f}s")
        
        self.history.add_user(user_text)
        self.history.add_assistant(out)
        self.last_manager_text = out
        self.last_parsed = parse_manager_payload(out)
        
        if not self.last_parsed.is_valid:
            print("\n‚ö†Ô∏è  VALIDATION WARNINGS:")
            for e in self.last_parsed.validation_errors:
                print(f"  - {e}")
                logger.warning(f"Validation: {e}")
        
        # FIX v4.2.1: Don't override manual targets
        if self.last_parsed.plan_ready and self.last_parsed.targets and not self.manual_target_set:
            self.targets = self.last_parsed.targets
            print(f"\nüéØ Targets: {', '.join(self.targets)}")
            
            complexity = TaskComplexity.analyze(
                self.last_parsed.spec,
                self.last_parsed.tasks,
                self.targets
            )
            
            print(f"üìä Task Complexity: {complexity.complexity_score:.1f}/100")
            print(f"   Files: {complexity.file_count}, Est. lines: ~{complexity.estimated_lines}, External deps: {'Yes' if complexity.has_external_deps else 'No'}")
            
            logger.info(f"Targets set: {self.targets}")
        
        return out
    
    def _extract_code_blocks(self, text: str) -> Dict[str, str]:
        """FIX v4.2.1: Better code extraction for incomplete blocks"""
        # Pattern to match complete code blocks
        pattern = r"```(?:python)?\n(.*?)```"
        blocks = re.findall(pattern, text, re.DOTALL)
        
        # FIX: Also try to extract incomplete blocks (missing closing ```)
        if not blocks or (len(blocks) == 1 and len(blocks[0].strip()) < 200):
            # Look for opening ``` without closing
            incomplete_pattern = r"```(?:python)?\n(.*?)$"
            incomplete_blocks = re.findall(incomplete_pattern, text, re.DOTALL | re.MULTILINE)
            if incomplete_blocks:
                # Use the incomplete block if it's substantial
                for block in incomplete_blocks:
                    if len(block.strip()) > 200:
                        blocks.append(block)
                        logger.warning(f"Extracted incomplete code block ({len(block)} chars)")
        
        if not blocks:
            logger.warning("No code blocks found in response")
            return {}
        
        # For single target, use the largest/most complete block
        if len(self.targets) == 1:
            # Filter out tiny blocks
            substantial_blocks = [b for b in blocks if len(b.strip()) > 100]
            
            if substantial_blocks:
                largest_block = max(substantial_blocks, key=len)
                logger.info(f"Using largest code block: {len(largest_block)} chars")
                return {self.targets[0]: largest_block.strip()}
            elif blocks:
                logger.warning("No substantial blocks found, using first block")
                return {self.targets[0]: blocks[0].strip()}
            else:
                return {}
        
        # For multiple files, try to match by filename mentions
        result = {}
        for target in self.targets:
            target_name = Path(target).name
            for i, block in enumerate(blocks):
                block_start = text.find(f"```python\n{block}")
                if block_start == -1:
                    block_start = text.find(f"```\n{block}")
                
                if block_start != -1:
                    context_before = text[max(0, block_start-500):block_start]
                    
                    if target_name in context_before:
                        result[target] = block.strip()
                        logger.info(f"Matched {target_name} to block {i}")
                        break
        
        # Fallback: distribute blocks to targets
        if not result:
            logger.warning("No filename matches, distributing blocks to targets")
            for i, target in enumerate(self.targets):
                if i < len(blocks):
                    result[target] = blocks[i].strip()
        
        return result
    
    def choose_model(self, user_preference: Optional[str] = None) -> str:
        """Choose model based on user preference or default"""
        if user_preference:
            pref_lower = user_preference.lower()
            if "glm" in pref_lower:
                logger.info("User selected GLM")
                return self.cfg.glm_model
            elif "qwen" in pref_lower:
                logger.info("User selected Qwen")
                return self.cfg.qwen_model
        
        logger.info(f"Using default model: {self.cfg.default_coder_model}")
        return self.cfg.default_coder_model
    
    def _analyze_file(self, filepath: Path) -> Dict:
        """Extract metadata from generated file"""
        try:
            code = filepath.read_text()
            
            # Extract function names
            functions = re.findall(r'^def (\w+)\(', code, re.MULTILINE)
            
            # Extract class names
            classes = re.findall(r'^class (\w+)', code, re.MULTILINE)
            
            # Extract imports
            imports = re.findall(r'^import (\w+)', code, re.MULTILINE)
            imports += re.findall(r'^from (\w+)', code, re.MULTILINE)
            
            return {
                'size': len(code),
                'functions': functions,
                'classes': classes,
                'dependencies': list(set(imports)),
                'exports': functions + classes
            }
        except Exception as e:
            logger.error(f"Error analyzing file {filepath}: {e}")
            return {
                'size': 0,
                'functions': [],
                'classes': [],
                'dependencies': [],
                'exports': []
            }
    
    def _extract_tasks_for_file(self, tasks: str, filename: str) -> str:
        """Extract relevant tasks for a specific file"""
        relevant = []
        for line in tasks.split('\n'):
            if filename in line:
                relevant.append(line)
        
        if relevant:
            return '\n'.join(relevant)
        else:
            return tasks
    
    def run_coder_sequential(self, spec, tasks, user_request="", model_preference=None):
        """FIX v4.2.1: Sequential file-by-file generation with proper path handling"""
        
        if not self.targets:
            return {"success": False, "error": "No target files"}
        
        # FIX v4.2.1: CRITICAL - Check manual_target_set FIRST before any manager logic
        first_target = Path(self.targets[0])
        
        # Convert to absolute path
        if not first_target.is_absolute():
            first_target = Path.cwd() / first_target
        
        # FIX v4.2.1: If user manually set target, ALWAYS respect it
        if self.manual_target_set:
            project_path = first_target.parent
            
            # Create folder if it doesn't exist
            if not project_path.exists():
                project_path.mkdir(parents=True, exist_ok=True)
                logger.info(f"Created target directory: {project_path}")
            
            print(f"\nüìÅ Using manually specified path: {project_path.name}")
            logger.info(f"Manual target path respected: {project_path}")
            self.project_manager.current_project = project_path
            
            # Update targets to be relative to project
            self.targets = [Path(t).name if Path(t).is_absolute() or '/' in t else t for t in self.targets]
        else:
            # Normal flow - create new project
            project_name = self.last_parsed.project_name if self.last_parsed else None
            project_path = self.project_manager.create_project(project_name, user_request)
            print(f"\nüìÅ Project: {project_path.name}")
        
        # Prerequisites check
        prereqs_ok, prereq_msgs = Prerequisites.validate_all(project_path, self.targets)
        print("\nüîç Prerequisites Check:")
        for msg in prereq_msgs:
            print(f"  {msg}")
        
        if not prereqs_ok:
            logger.error("Prerequisites check failed")
            return {"success": False, "error": "Prerequisites check failed", "prereqs": prereq_msgs}
        
        # Initialize context report (skip if manual target to existing project)
        context_mgr = ContextReportManager(project_path)
        
        # Only initialize if new project or no existing context report
        if not self.manual_target_set or not context_mgr.report_path.exists():
            context_mgr.initialize(
                self.last_parsed.project_name if self.last_parsed else "project",
                spec,
                self.targets
            )
            print(f"\nüìã Context Report: {project_path.name}/CONTEXT_REPORT.md")
        else:
            print(f"\nüìã Using existing context report: {context_mgr.report_path.name}")
        
        print(f"üîÑ Sequential Generation: {len(self.targets)} files")
        
        # Choose model
        chosen_model = self.choose_model(model_preference)
        self.current_model = chosen_model
        model_name = "GLM" if "glm" in chosen_model.lower() else "Qwen"
        print(f"ü§ñ Using {model_name} ({chosen_model})")
        
        # Generate files sequentially
        all_results = []
        
        for idx, target_file in enumerate(self.targets, 1):
            print(f"\n{'='*70}")
            print(f"üìù File {idx}/{len(self.targets)}: {target_file}")
            print(f"{'='*70}")
            
            # Get context from previous files
            file_context = context_mgr.get_context_for_file(target_file)
            
            # Build file-specific prompt
            file_tasks = self._extract_tasks_for_file(tasks, target_file)
            
            prompt = f"""You are generating file {idx} of {len(self.targets)} for a project.

OVERALL SPECIFICATION:
{spec}

TASKS FOR THIS FILE ({target_file}):
{file_tasks}

{file_context}

TARGET FILE: {target_file}

INSTRUCTIONS:
- Generate ONLY {target_file} (complete implementation)
- Import from already-completed modules as needed
- Maintain consistency with existing code patterns
- Include comprehensive error handling
- Add docstrings and type hints
- Put ALL code for {target_file} in a SINGLE ```python code block
- Make it production-ready and fully functional

Generate the complete {target_file} now:"""
            
            # Generate this file
            print(f"üîß Generating {target_file}...")
            t0 = now_s()
            
            try:
                print("\n--- CODER OUTPUT ---")
                full_response = []
                for chunk in self.coder.generate_stream(chosen_model, prompt, timeout=300):
                    print(chunk, end='', flush=True)
                    full_response.append(chunk)
                
                response_text = ''.join(full_response)
                print("\n--- END OUTPUT ---\n")
                
                elapsed = now_s() - t0
                
                # Extract code (temporarily set targets to single file)
                original_targets = self.targets.copy()
                self.targets = [target_file]
                code_blocks = self._extract_code_blocks(response_text)
                self.targets = original_targets
                
                if target_file not in code_blocks:
                    print(f"‚ùå Failed to extract code for {target_file}")
                    logger.error(f"No code block found for {target_file}")
                    all_results.append({"file": target_file, "success": False, "error": "No code block found"})
                    continue
                
                # Write file
                filepath = project_path / target_file
                code = code_blocks[target_file]
                filepath.write_text(code)
                
                print(f"‚úì Wrote {target_file} ({len(code)} chars, {elapsed:.1f}s)")
                logger.info(f"Generated {target_file}: {len(code)} chars in {elapsed:.1f}s")
                
                # Analyze file
                file_info = self._analyze_file(filepath)
                
                # Update context report
                context_mgr.mark_file_complete(target_file, file_info)
                
                # Add integration note
                if file_info['dependencies']:
                    note = f"{target_file} imports: {', '.join(file_info['dependencies'][:3])}"
                    context_mgr.add_integration_note(note)
                
                all_results.append({
                    "file": target_file,
                    "success": True,
                    "size": len(code),
                    "time_s": elapsed,
                    "functions": len(file_info['functions']),
                    "classes": len(file_info['classes'])
                })
                
                # Show progress
                completed = idx
                remaining = len(self.targets) - completed
                print(f"\n‚úÖ Progress: {completed}/{len(self.targets)} complete")
                if remaining > 0:
                    print(f"   Next: {self.targets[idx] if idx < len(self.targets) else 'None'}")
                print(f"   Context: {context_mgr.report_path.name}")
                
            except Exception as e:
                print(f"\n‚ùå Error generating {target_file}: {e}")
                logger.exception(f"Error generating {target_file}")
                all_results.append({"file": target_file, "success": False, "error": str(e)})
        
        # Final summary
        print(f"\n{'='*70}")
        print(f"üìä GENERATION COMPLETE")
        print(f"{'='*70}")
        
        success_count = sum(1 for r in all_results if r.get("success"))
        total_time = sum(r.get("time_s", 0) for r in all_results if r.get("success"))
        total_size = sum(r.get("size", 0) for r in all_results if r.get("success"))
        
        print(f"‚úÖ Successful: {success_count}/{len(self.targets)} files")
        print(f"‚è±Ô∏è  Total Time: {total_time:.1f}s")
        print(f"üì¶ Total Code: {total_size:,} chars")
        print(f"üìã Context Report: {context_mgr.report_path}")
        
        # Update README
        successful_files = [r['file'] for r in all_results if r.get('success')]
        self.project_manager.update_readme(successful_files)
        
        # Run integration tests
        test_runner = TestRunner(project_path)
        if test_runner.has_tests():
            print(f"\nüß™ Running integration tests...")
            test_success, test_output = test_runner.run_tests()
            if test_success:
                print("‚úÖ All tests passed!")
                context_mgr.add_integration_note("All integration tests passed")
            else:
                print("‚ö†Ô∏è  Some tests failed")
                print(test_output[:500])
                context_mgr.add_integration_note("Integration tests: Some failures")
            
            self.project_manager.update_readme(successful_files, test_output)
        else:
            print("üìù No tests found")
        
        return {
            "success": success_count == len(self.targets),
            "files_generated": success_count,
            "total_files": len(self.targets),
            "results": all_results,
            "project_path": str(project_path),
            "context_report": str(context_mgr.report_path),
            "total_time_s": total_time,
            "total_size": total_size
        }
    
    def refine_code(self, feedback: str) -> Dict:
        """Iterative refinement based on manager feedback"""
        if not self.project_manager.current_project:
            return {"success": False, "error": "No active project to refine"}
        
        self.refinement_round += 1
        print(f"\nüîÑ Refinement Round {self.refinement_round}")
        
        current_code = {}
        for target in self.targets:
            filepath = self.project_manager.get_project_path(target)
            if filepath.exists():
                current_code[target] = filepath.read_text()
        
        files_content = "\n\n".join([f"# {fname}\n```python\n{code}\n```" for fname, code in current_code.items()])
        
        prompt = f"""You are refining existing code based on feedback.

CURRENT CODE:
{files_content}

FEEDBACK:
{feedback}

INSTRUCTIONS:
- Address all points in the feedback
- Maintain existing functionality
- Improve code quality, error handling, documentation
- Put refined code in ```python blocks with filenames
- Only include files that need changes

Generate the refined code now."""

        print(f"üîß Refining with {self.current_model}...")
        t0 = now_s()
        
        try:
            print("\n--- REFINEMENT OUTPUT ---")
            full_response = []
            for chunk in self.coder.generate_stream(self.current_model, prompt, timeout=300):
                print(chunk, end='', flush=True)
                full_response.append(chunk)
            
            response_text = ''.join(full_response)
            print("\n--- END OUTPUT ---\n")
            
            elapsed = now_s() - t0
            
            code_blocks = self._extract_code_blocks(response_text)
            
            if not code_blocks:
                return {"success": False, "error": "No refined code found"}
            
            updated_files = []
            for filename, code in code_blocks.items():
                filepath = self.project_manager.get_project_path(filename)
                filepath.write_text(code)
                updated_files.append(filename)
                print(f"  ‚úì Updated {filename}")
            
            test_runner = TestRunner(self.project_manager.current_project)
            test_success, test_output = test_runner.run_tests()
            
            if test_runner.has_tests():
                print(f"\nüß™ Re-running tests...")
                if test_success:
                    print("‚úÖ All tests passed after refinement!")
                else:
                    print("‚ö†Ô∏è  Some tests still failing")
            
            return {
                "success": True,
                "time_s": elapsed,
                "files_updated": updated_files,
                "tests_passed": test_success if test_runner.has_tests() else None,
                "round": self.refinement_round
            }
            
        except Exception as e:
            logger.error(f"Refinement error: {e}")
            return {"success": False, "error": str(e)}
    
    def review(self, spec, output):
        print("\nüß† Manager reviewing...")
        logger.info("Manager reviewing output")
        prompt = f"""Review coder output vs spec. If complete, give checklist. If issues, list them.

SPEC:\n{spec}\n\nOUTPUT:\n{output[:6000]}"""
        return self.manager.generate(self.cfg.manager_model, prompt, 300)

    def run_ralph_loop(self, task: str, max_iterations: int = 20, model_preference: str = None) -> Dict:
        """
        Run an autonomous Ralph Wiggum loop until task completion or safety valve triggers.

        The loop:
        1. Reads progress state (progress.txt, prd.json)
        2. Executes one iteration of work
        3. Checks for completion or errors
        4. Updates state and repeats
        """

        # Ensure we have a project
        if not self.project_manager.current_project:
            # Create a new project for this Ralph loop
            project_name = f"ralph_loop_{sanitize_folder_name(task[:30])}"
            project_path = self.project_manager.create_project(project_name, task)
            print(f"\nüìÅ Created Ralph Loop project: {project_path.name}")
        else:
            project_path = self.project_manager.current_project
            print(f"\nüìÅ Using existing project: {project_path.name}")

        # Initialize Ralph loop
        ralph = RalphWiggumLoop(project_path, max_iterations)

        # Initialize progress tracking files
        print(f"\nüîÑ Initializing Ralph Wiggum Loop")
        print(f"   Task: {task}")
        print(f"   Max Iterations: {max_iterations}")
        print(f"   Safety Valve: Enabled")

        ralph.initialize_progress(task)

        # Choose model
        chosen_model = self.choose_model(model_preference)
        self.current_model = chosen_model
        model_name = "GLM" if "glm" in chosen_model.lower() else "Qwen"
        print(f"ü§ñ Using {model_name} ({chosen_model})")

        # Main Ralph loop
        completed = False
        final_result = None

        print(f"\n{'='*70}")
        print(f"üåÄ RALPH WIGGUM LOOP STARTING")
        print(f"{'='*70}")

        while not ralph.check_safety_valve() and not completed:
            ralph.current_iteration += 1

            print(f"\n{'‚îÄ'*70}")
            print(f"üîÅ Iteration {ralph.current_iteration}/{ralph.max_iterations}")
            print(f"{'‚îÄ'*70}")

            # Get current context
            context = ""
            if self.last_parsed:
                context = f"SPEC:\n{self.last_parsed.spec}\n\nTASKS:\n{self.last_parsed.tasks}"

            # Generate loop prompt
            prompt = ralph.get_loop_prompt(task, context)

            # Execute iteration
            try:
                print(f"üîß Executing iteration...")
                t0 = now_s()

                print("\n--- RALPH OUTPUT ---")
                full_response = []
                for chunk in self.coder.generate_stream(chosen_model, prompt, timeout=300):
                    print(chunk, end='', flush=True)
                    full_response.append(chunk)

                response_text = ''.join(full_response)
                print("\n--- END OUTPUT ---\n")

                elapsed = now_s() - t0
                print(f"‚è±Ô∏è  Iteration completed in {elapsed:.1f}s")

                # Check for completion promise
                if RalphWiggumLoop.PROMISE_COMPLETE in response_text:
                    print(f"\n‚úÖ COMPLETION PROMISE DETECTED!")
                    completed = True

                    # Verify by checking PRD
                    _, prd_data = ralph.read_progress()
                    if ralph.is_complete(prd_data):
                        print(f"‚úÖ PRD verification: All tasks complete!")
                    else:
                        print(f"‚ö†Ô∏è  PRD verification: Some tasks still incomplete")
                        # Continue loop if PRD not actually complete
                        completed = False
                        ralph.update_progress(
                            "Premature completion claim",
                            "AI claimed complete but PRD not 100%",
                            "PRD tasks still pending",
                            "VERIFICATION_FAILED"
                        )
                        continue

                # Extract and apply code if present
                code_blocks = self._extract_code_blocks(response_text)
                if code_blocks:
                    for filename, code in code_blocks.items():
                        filepath = project_path / filename
                        filepath.parent.mkdir(parents=True, exist_ok=True)
                        filepath.write_text(code)
                        print(f"  üìù Wrote: {filename}")

                # Run tests if available
                test_runner = TestRunner(project_path)
                if test_runner.has_tests():
                    print(f"\nüß™ Running tests...")
                    test_success, test_output = test_runner.run_tests()
                    if test_success:
                        print(f"‚úÖ Tests passed!")
                        ralph.update_progress(
                            "Tests passing",
                            "All tests passed",
                            "None",
                            "TESTS_PASSING"
                        )
                    else:
                        print(f"‚ùå Tests failed - will retry")
                        # Extract failure summary
                        failure_lines = [l for l in test_output.split('\n') if 'FAILED' in l or 'Error' in l]
                        blocker = '; '.join(failure_lines[:3]) if failure_lines else "Test failures"
                        ralph.update_progress(
                            "Tests failing",
                            "Tests failed, analyzing for retry",
                            blocker[:200],
                            "TESTS_FAILING"
                        )
                else:
                    ralph.update_progress(
                        "No tests to run",
                        "Iteration completed, no test files found",
                        "None",
                        "WORKING"
                    )

                # Parse any progress updates from the response
                if "Progress update:" in response_text.lower() or "status:" in response_text.lower():
                    # The AI provided a status update, log it
                    logger.info(f"Iteration {ralph.current_iteration} completed with status update")

            except Exception as e:
                print(f"\n‚ùå Iteration error: {e}")
                logger.exception(f"Ralph loop iteration {ralph.current_iteration} failed")
                ralph.update_progress(
                    "Error occurred",
                    f"Exception: {str(e)[:100]}",
                    str(e)[:200],
                    "ERROR"
                )
                # Continue to next iteration (persistence!)

        # Loop complete - summarize
        print(f"\n{'='*70}")
        print(f"üèÅ RALPH WIGGUM LOOP FINISHED")
        print(f"{'='*70}")

        _, final_prd = ralph.read_progress()
        completion_pct = final_prd.get('completion_percentage', 0) if final_prd else 0

        if completed:
            print(f"‚úÖ Status: COMPLETED")
            print(f"üìä PRD Completion: {completion_pct}%")
        elif ralph.check_safety_valve():
            print(f"‚ö†Ô∏è  Status: SAFETY VALVE TRIGGERED")
            print(f"üìä PRD Completion: {completion_pct}%")
            print(f"üí° Tip: Review progress.txt and continue manually or increase --max-iterations")
        else:
            print(f"‚ùì Status: UNKNOWN EXIT")

        print(f"üîÅ Total Iterations: {ralph.current_iteration}")
        print(f"üìÅ Project: {project_path}")
        print(f"üìã Progress: {ralph.progress_path}")
        print(f"üìÑ PRD: {ralph.prd_path}")

        return {
            "success": completed,
            "iterations": ralph.current_iteration,
            "max_iterations": ralph.max_iterations,
            "safety_valve_triggered": ralph.check_safety_valve(),
            "completion_percentage": completion_pct,
            "project_path": str(project_path),
            "progress_file": str(ralph.progress_path),
            "prd_file": str(ralph.prd_path)
        }
    
    def interactive(self):
        print("\n" + "="*78)
        print("üöÄ COLLABORATIVE AGENT v4.2.1 PROFESSIONAL EDITION")
        print("="*78)
        print("FIXED: Manual :target paths now properly respected!")
        print("")
        print("Commands:")
        print("  execute             - Run sequential generation (default model)")
        print("  execute glm         - Run with GLM")
        print("  execute qwen        - Run with Qwen")
        print("  ralph <task>        - Start autonomous Ralph Wiggum loop")
        print("  ralph --max 30      - Ralph loop with custom max iterations")
        print("  refine              - Improve code based on feedback")
        print("  manager             - Show last manager response")
        print("  templates           - List available templates")
        print("  template <name>     - Use a template")
        print("  :target file.py     - Set target files manually")
        print("  :history            - Show conversation history")
        print("  :clear              - Clear conversation")
        print("  quit                - Exit")
        print("")
        print("Ralph Wiggum Loop:")
        print("  The Ralph loop is an autonomous agent mode that iterates until")
        print("  the task is verified complete. It tracks progress via progress.txt")
        print("  and prd.json, and only exits when <promise>COMPLETE</promise> is")
        print("  reached or the safety valve (max iterations) triggers.")
        print("="*78)
        logger.info("Starting interactive session")
        
        last_user_request = ""
        
        while True:
            try:
                inp = input("\nüí¨ You: ").strip()
                if not inp:
                    continue
                
                if inp in ["quit", "exit", "q"]:
                    logger.info("User quit")
                    return 0
                
                if inp == "templates":
                    templates = self.template_library.list_templates()
                    print("\nüìÑ Available Templates:")
                    for t in templates:
                        print(f"  ‚Ä¢ {t['name']}: {t['description']} ({t['complexity']})")
                    continue
                
                if inp.startswith("template "):
                    template_name = inp.split(maxsplit=1)[1]
                    template = self.template_library.get_template(template_name)
                    if template:
                        print(f"\nüìÑ Template: {template['name']}")
                        print(f"Description: {template['description']}")
                        print(f"Complexity: {template['complexity']}")
                        print(f"Files: {', '.join(template['files'])}")
                        
                        params = {}
                        for match in re.finditer(r'\{(\w+)\}', template['prompt_template']):
                            param_name = match.group(1)
                            value = input(f"  {param_name}: ")
                            params[param_name] = value
                        
                        prompt = self.template_library.apply_template(template_name, **params)
                        if prompt:
                            print(f"\n‚úì Using template. Asking manager...")
                            last_user_request = prompt
                            out = self.ask_manager(prompt)
                            print(f"\nü§ñ Manager:\n{out}")
                    else:
                        print(f"‚ùå Template not found: {template_name}")
                    continue
                
                if inp.startswith(":target "):
                    self.targets = [f.strip() for f in inp.split()[1:]]
                    self.manual_target_set = True  # FIX v4.2.1: Set flag
                    print(f"‚úì Targets: {self.targets}")
                    logger.info(f"Manual targets set: {self.targets}")
                    continue
                
                if inp == ":history":
                    for t in self.history.turns:
                        print(f"\n{t.role.upper()}: {t.content[:200]}")
                    continue
                
                if inp == ":clear":
                    self.history.clear()
                    self.refinement_round = 0
                    self.manual_target_set = False  # FIX v4.2.1: Reset flag
                    print("‚úì Cleared")
                    continue
                
                if inp == "manager":
                    print(f"\nü§ñ {self.last_manager_text or '(none)'}")
                    continue
                
                if inp.startswith("ralph"):
                    # Parse ralph command: ralph <task> [--max N] [--model glm/qwen]
                    parts = inp.split()
                    max_iterations = 20
                    model_pref = None
                    task_parts = []

                    i = 1
                    while i < len(parts):
                        if parts[i] in ('--max', '--max-iterations', '-m'):
                            if i + 1 < len(parts):
                                try:
                                    max_iterations = int(parts[i + 1])
                                    i += 2
                                    continue
                                except ValueError:
                                    print(f"‚ùå Invalid max iterations: {parts[i + 1]}")
                                    break
                        elif parts[i] in ('--model', '-M'):
                            if i + 1 < len(parts):
                                model_pref = parts[i + 1]
                                i += 2
                                continue
                        else:
                            task_parts.append(parts[i])
                        i += 1

                    task = ' '.join(task_parts)

                    if not task:
                        # Prompt for task if not provided
                        task = input("\nüí¨ What task should Ralph work on? ").strip()

                    if not task:
                        print("‚ùå No task provided for Ralph loop")
                        continue

                    # Confirm before starting
                    print(f"\nüåÄ Starting Ralph Wiggum Loop")
                    print(f"   Task: {task}")
                    print(f"   Max Iterations: {max_iterations}")
                    print(f"   Model: {model_pref or 'default'}")
                    confirm = input("\n   Start loop? (y/n): ").strip().lower()

                    if confirm not in ('y', 'yes'):
                        print("   Cancelled.")
                        continue

                    # First, get a plan from the manager if we don't have one
                    if not self.last_parsed or not self.last_parsed.plan_ready:
                        print("\nüìã Getting plan from manager first...")
                        self.ask_manager(task)

                    result = self.run_ralph_loop(task, max_iterations, model_pref)

                    if result.get('success'):
                        print(f"\nüéâ Ralph loop completed successfully!")
                    elif result.get('safety_valve_triggered'):
                        print(f"\n‚ö†Ô∏è  Ralph loop hit safety valve after {result['iterations']} iterations")
                        print(f"   Progress: {result.get('completion_percentage', 0)}%")
                        print(f"   Review {result['progress_file']} for status")
                    else:
                        print(f"\n‚ùì Ralph loop ended unexpectedly")

                    continue

                if inp.startswith("execute"):
                    parts = inp.split()
                    model_pref = parts[1] if len(parts) > 1 else None

                    # FIX v4.2.1: Allow execution even without manager if manual target set
                    if not self.manual_target_set:
                        if not self.last_parsed:
                            print("‚ùå No plan yet")
                            logger.warning("Execute called without plan")
                            continue
                        if not self.last_parsed.plan_ready:
                            print("‚ùå PLAN_READY is NO")
                            logger.warning("Execute called but plan not ready")
                            continue
                        if not self.last_parsed.is_valid:
                            print("‚ùå Validation errors - ask manager to fix")
                            logger.warning("Execute called with validation errors")
                            continue
                    
                    self.refinement_round = 0
                    
                    # Use sequential generation
                    result = self.run_coder_sequential(
                        self.last_parsed.spec if self.last_parsed else "",
                        self.last_parsed.tasks if self.last_parsed else "",
                        user_request=last_user_request,
                        model_preference=model_pref
                    )
                    
                    success_emoji = '‚úÖ' if result.get('success') else '‚ö†Ô∏è'
                    print(f"\n{success_emoji} {'SUCCESS' if result.get('success') else 'PARTIAL SUCCESS'}")
                    
                    if result.get("files_generated"):
                        print(f"üìù Generated: {result['files_generated']}/{result['total_files']} files")
                    if result.get("total_time_s"):
                        print(f"‚è±Ô∏è  Total Time: {result['total_time_s']:.1f}s")
                    if result.get("total_size"):
                        print(f"üì¶ Code Size: {result['total_size']:,} chars")
                    if result.get("project_path"):
                        print(f"üìÅ Project: {result['project_path']}")
                    if result.get("context_report"):
                        print(f"üìã Context: {result['context_report']}")
                    if result.get("error"):
                        print(f"‚ùå Error: {result['error']}")
                    
                    # FIX v4.2.1: Reset manual flag after execution
                    self.manual_target_set = False
                    continue
                
                if inp == "refine":
                    if not self.project_manager.current_project:
                        print("‚ùå No active project. Run 'execute' first.")
                        continue
                    
                    feedback = input("\nüí¨ Feedback for refinement: ").strip()
                    if not feedback:
                        print("‚ùå No feedback provided")
                        continue
                    
                    result = self.refine_code(feedback)
                    
                    if result.get("success"):
                        print(f"\n‚úÖ Refinement complete (Round {result['round']})")
                        print(f"‚è±Ô∏è  {result['time_s']:.1f}s")
                        print(f"üìù Updated: {', '.join(result['files_updated'])}")
                        if result.get("tests_passed") is not None:
                            test_emoji = '‚úÖ' if result['tests_passed'] else '‚ö†Ô∏è'
                            print(f"{test_emoji} Tests: {'Passed' if result['tests_passed'] else 'Failed'}")
                    else:
                        print(f"‚ùå Refinement failed: {result.get('error')}")
                    continue
                
                # Regular user request
                last_user_request = inp
                self.manual_target_set = False  # FIX v4.2.1: Reset for new request
                out = self.ask_manager(inp)
                print(f"\nü§ñ Manager:\n{out}")
                
            except KeyboardInterrupt:
                print("\nüëã Bye")
                logger.info("Interrupted by user")
                return 0
            except Exception as e:
                print(f"\n‚ùå Error: {e}")
                logger.exception("Unexpected error")
                continue

if __name__ == "__main__":
    raise SystemExit(CollaborativeAgentPro().interactive())
